This file contains the output and error log of the command found in finetune_command.txt


starting check cuda status True
local ip:  172.28.1.12
make sure the distributed mode is  tcp://localhost:10001
| distributed init (rank 2): tcp://localhost:10001, gpu 2
make sure the distributed mode is  tcp://localhost:10001
| distributed init (rank 0): tcp://localhost:10001, gpu 0
make sure the distributed mode is  tcp://localhost:10001
| distributed init (rank 1): tcp://localhost:10001, gpu 1
[12:01:27.284375] job dir: /storage/ixjl/cse428/finetune
[12:01:27.284471] Namespace(batch_size=4,
accum_iter=2,
epochs=32,
warmup_epochs=5,
start_epoch=0,
lr=None,
blr=0.0015,
min_lr=0.0,
weight_decay=0.05,
layer_decay=0.75,
model='vit_large_patch16',
pretrain='/storage/ixjl/cse428/hicfoundation_model/hicfoundation_pretrain.pth.tar',
resume='',
finetune=0,
seed=888,
loss_type=3,
data_path='/storage/ixjl/cse428/processed_finetune_data_multiple_regions',
train_config='/storage/ixjl/cse428/processed_finetune_data_multiple_regions/train.txt',
valid_config='/storage/ixjl/cse428/processed_finetune_data_multiple_regions/val.txt',
output='./output_may_30',
tensorboard=0,
device='cuda',
num_workers=8,
pin_mem=False,
world_size=3,
dist_url='tcp://localhost:10001',
rank=0,
input_row_size=224,
input_col_size=224,
patch_size=16,
print_freq=1,
save_freq=1,
gpu=0,
patience=10,
distributed=True,
dist_backend='nccl')
[12:01:27.286895] The number of samples used in the dataset is 12
[12:01:27.288499] The number of samples used in the dataset is 10
[12:01:27.288570] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fae5c92ec40>
[12:01:27.288666] Data loader is configured!
[12:01:35.626887] Loading pre-trained model from /storage/ixjl/cse428/hicfoundation_model/hicfoundation_pretrain.pth.tar
[12:01:36.318438] DEBUG: loaded checkpoint pos_embed shape → torch.Size([1, 197, 1024])
[12:01:36.332743] Expanded pos_embed from 197 → 199 (CLS + COUNT + SEQ + patches)
[12:01:36.334732] DEBUG → num_extra_tokens: 3 (expected 3)
[12:01:38.819771] ==================================================
[12:01:38.819804] Important:Loading pre-trained encoder message: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias', 'sequence_encoder.conv1.weight', 'sequence_encoder.conv1.bias', 'sequence_encoder.conv2.weight', 'sequence_encoder.conv2.bias', 'sequence_encoder.conv3.weight', 'sequence_encoder.conv3.bias', 'sequence_encoder.fc.weight', 'sequence_encoder.fc.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias', 'decoder_count.weight', 'decoder_count.bias'])
[12:01:38.819810] ==================================================
[12:01:39.072736] Loading pre-trained model from /storage/ixjl/cse428/hicfoundation_model/hicfoundation_pretrain.pth.tar for decoder
[12:01:39.779356] DEBUG: loaded checkpoint pos_embed shape → torch.Size([1, 197, 1024])
[12:01:39.790344] Expanded pos_embed from 197 → 199 (CLS + COUNT + SEQ + patches)
[12:01:39.836294] ==================================================
[12:01:39.836333] Important:Loading pre-trained decoder message: _IncompatibleKeys(missing_keys=['vit_backbone.cls_token', 'vit_backbone.pos_embed', 'vit_backbone.patch_embed.proj.weight', 'vit_backbone.patch_embed.proj.bias', 'vit_backbone.blocks.0.norm1.weight', 'vit_backbone.blocks.0.norm1.bias', 'vit_backbone.blocks.0.attn.qkv.weight', 'vit_backbone.blocks.0.attn.qkv.bias', 'vit_backbone.blocks.0.attn.proj.weight', 'vit_backbone.blocks.0.attn.proj.bias', 'vit_backbone.blocks.0.norm2.weight', 'vit_backbone.blocks.0.norm2.bias', 'vit_backbone.blocks.0.mlp.fc1.weight', 'vit_backbone.blocks.0.mlp.fc1.bias', 'vit_backbone.blocks.0.mlp.fc2.weight', 'vit_backbone.blocks.0.mlp.fc2.bias', 'vit_backbone.blocks.1.norm1.weight', 'vit_backbone.blocks.1.norm1.bias', 'vit_backbone.blocks.1.attn.qkv.weight', 'vit_backbone.blocks.1.attn.qkv.bias', 'vit_backbone.blocks.1.attn.proj.weight', 'vit_backbone.blocks.1.attn.proj.bias', 'vit_backbone.blocks.1.norm2.weight', 'vit_backbone.blocks.1.norm2.bias', 'vit_backbone.blocks.1.mlp.fc1.weight', 'vit_backbone.blocks.1.mlp.fc1.bias', 'vit_backbone.blocks.1.mlp.fc2.weight', 'vit_backbone.blocks.1.mlp.fc2.bias', 'vit_backbone.blocks.2.norm1.weight', 'vit_backbone.blocks.2.norm1.bias', 'vit_backbone.blocks.2.attn.qkv.weight', 'vit_backbone.blocks.2.attn.qkv.bias', 'vit_backbone.blocks.2.attn.proj.weight', 'vit_backbone.blocks.2.attn.proj.bias', 'vit_backbone.blocks.2.norm2.weight', 'vit_backbone.blocks.2.norm2.bias', 'vit_backbone.blocks.2.mlp.fc1.weight', 'vit_backbone.blocks.2.mlp.fc1.bias', 'vit_backbone.blocks.2.mlp.fc2.weight', 'vit_backbone.blocks.2.mlp.fc2.bias', 'vit_backbone.blocks.3.norm1.weight', 'vit_backbone.blocks.3.norm1.bias', 'vit_backbone.blocks.3.attn.qkv.weight', 'vit_backbone.blocks.3.attn.qkv.bias', 'vit_backbone.blocks.3.attn.proj.weight', 'vit_backbone.blocks.3.attn.proj.bias', 'vit_backbone.blocks.3.norm2.weight', 'vit_backbone.blocks.3.norm2.bias', 'vit_backbone.blocks.3.mlp.fc1.weight', 'vit_backbone.blocks.3.mlp.fc1.bias', 'vit_backbone.blocks.3.mlp.fc2.weight', 'vit_backbone.blocks.3.mlp.fc2.bias', 'vit_backbone.blocks.4.norm1.weight', 'vit_backbone.blocks.4.norm1.bias', 'vit_backbone.blocks.4.attn.qkv.weight', 'vit_backbone.blocks.4.attn.qkv.bias', 'vit_backbone.blocks.4.attn.proj.weight', 'vit_backbone.blocks.4.attn.proj.bias', 'vit_backbone.blocks.4.norm2.weight', 'vit_backbone.blocks.4.norm2.bias', 'vit_backbone.blocks.4.mlp.fc1.weight', 'vit_backbone.blocks.4.mlp.fc1.bias', 'vit_backbone.blocks.4.mlp.fc2.weight', 'vit_backbone.blocks.4.mlp.fc2.bias', 'vit_backbone.blocks.5.norm1.weight', 'vit_backbone.blocks.5.norm1.bias', 'vit_backbone.blocks.5.attn.qkv.weight', 'vit_backbone.blocks.5.attn.qkv.bias', 'vit_backbone.blocks.5.attn.proj.weight', 'vit_backbone.blocks.5.attn.proj.bias', 'vit_backbone.blocks.5.norm2.weight', 'vit_backbone.blocks.5.norm2.bias', 'vit_backbone.blocks.5.mlp.fc1.weight', 'vit_backbone.blocks.5.mlp.fc1.bias', 'vit_backbone.blocks.5.mlp.fc2.weight', 'vit_backbone.blocks.5.mlp.fc2.bias', 'vit_backbone.blocks.6.norm1.weight', 'vit_backbone.blocks.6.norm1.bias', 'vit_backbone.blocks.6.attn.qkv.weight', 'vit_backbone.blocks.6.attn.qkv.bias', 'vit_backbone.blocks.6.attn.proj.weight', 'vit_backbone.blocks.6.attn.proj.bias', 'vit_backbone.blocks.6.norm2.weight', 'vit_backbone.blocks.6.norm2.bias', 'vit_backbone.blocks.6.mlp.fc1.weight', 'vit_backbone.blocks.6.mlp.fc1.bias', 'vit_backbone.blocks.6.mlp.fc2.weight', 'vit_backbone.blocks.6.mlp.fc2.bias', 'vit_backbone.blocks.7.norm1.weight', 'vit_backbone.blocks.7.norm1.bias', 'vit_backbone.blocks.7.attn.qkv.weight', 'vit_backbone.blocks.7.attn.qkv.bias', 'vit_backbone.blocks.7.attn.proj.weight', 'vit_backbone.blocks.7.attn.proj.bias', 'vit_backbone.blocks.7.norm2.weight', 'vit_backbone.blocks.7.norm2.bias', 'vit_backbone.blocks.7.mlp.fc1.weight', 'vit_backbone.blocks.7.mlp.fc1.bias', 'vit_backbone.blocks.7.mlp.fc2.weight', 'vit_backbone.blocks.7.mlp.fc2.bias', 'vit_backbone.blocks.8.norm1.weight', 'vit_backbone.blocks.8.norm1.bias', 'vit_backbone.blocks.8.attn.qkv.weight', 'vit_backbone.blocks.8.attn.qkv.bias', 'vit_backbone.blocks.8.attn.proj.weight', 'vit_backbone.blocks.8.attn.proj.bias', 'vit_backbone.blocks.8.norm2.weight', 'vit_backbone.blocks.8.norm2.bias', 'vit_backbone.blocks.8.mlp.fc1.weight', 'vit_backbone.blocks.8.mlp.fc1.bias', 'vit_backbone.blocks.8.mlp.fc2.weight', 'vit_backbone.blocks.8.mlp.fc2.bias', 'vit_backbone.blocks.9.norm1.weight', 'vit_backbone.blocks.9.norm1.bias', 'vit_backbone.blocks.9.attn.qkv.weight', 'vit_backbone.blocks.9.attn.qkv.bias', 'vit_backbone.blocks.9.attn.proj.weight', 'vit_backbone.blocks.9.attn.proj.bias', 'vit_backbone.blocks.9.norm2.weight', 'vit_backbone.blocks.9.norm2.bias', 'vit_backbone.blocks.9.mlp.fc1.weight', 'vit_backbone.blocks.9.mlp.fc1.bias', 'vit_backbone.blocks.9.mlp.fc2.weight', 'vit_backbone.blocks.9.mlp.fc2.bias', 'vit_backbone.blocks.10.norm1.weight', 'vit_backbone.blocks.10.norm1.bias', 'vit_backbone.blocks.10.attn.qkv.weight', 'vit_backbone.blocks.10.attn.qkv.bias', 'vit_backbone.blocks.10.attn.proj.weight', 'vit_backbone.blocks.10.attn.proj.bias', 'vit_backbone.blocks.10.norm2.weight', 'vit_backbone.blocks.10.norm2.bias', 'vit_backbone.blocks.10.mlp.fc1.weight', 'vit_backbone.blocks.10.mlp.fc1.bias', 'vit_backbone.blocks.10.mlp.fc2.weight', 'vit_backbone.blocks.10.mlp.fc2.bias', 'vit_backbone.blocks.11.norm1.weight', 'vit_backbone.blocks.11.norm1.bias', 'vit_backbone.blocks.11.attn.qkv.weight', 'vit_backbone.blocks.11.attn.qkv.bias', 'vit_backbone.blocks.11.attn.proj.weight', 'vit_backbone.blocks.11.attn.proj.bias', 'vit_backbone.blocks.11.norm2.weight', 'vit_backbone.blocks.11.norm2.bias', 'vit_backbone.blocks.11.mlp.fc1.weight', 'vit_backbone.blocks.11.mlp.fc1.bias', 'vit_backbone.blocks.11.mlp.fc2.weight', 'vit_backbone.blocks.11.mlp.fc2.bias', 'vit_backbone.blocks.12.norm1.weight', 'vit_backbone.blocks.12.norm1.bias', 'vit_backbone.blocks.12.attn.qkv.weight', 'vit_backbone.blocks.12.attn.qkv.bias', 'vit_backbone.blocks.12.attn.proj.weight', 'vit_backbone.blocks.12.attn.proj.bias', 'vit_backbone.blocks.12.norm2.weight', 'vit_backbone.blocks.12.norm2.bias', 'vit_backbone.blocks.12.mlp.fc1.weight', 'vit_backbone.blocks.12.mlp.fc1.bias', 'vit_backbone.blocks.12.mlp.fc2.weight', 'vit_backbone.blocks.12.mlp.fc2.bias', 'vit_backbone.blocks.13.norm1.weight', 'vit_backbone.blocks.13.norm1.bias', 'vit_backbone.blocks.13.attn.qkv.weight', 'vit_backbone.blocks.13.attn.qkv.bias', 'vit_backbone.blocks.13.attn.proj.weight', 'vit_backbone.blocks.13.attn.proj.bias', 'vit_backbone.blocks.13.norm2.weight', 'vit_backbone.blocks.13.norm2.bias', 'vit_backbone.blocks.13.mlp.fc1.weight', 'vit_backbone.blocks.13.mlp.fc1.bias', 'vit_backbone.blocks.13.mlp.fc2.weight', 'vit_backbone.blocks.13.mlp.fc2.bias', 'vit_backbone.blocks.14.norm1.weight', 'vit_backbone.blocks.14.norm1.bias', 'vit_backbone.blocks.14.attn.qkv.weight', 'vit_backbone.blocks.14.attn.qkv.bias', 'vit_backbone.blocks.14.attn.proj.weight', 'vit_backbone.blocks.14.attn.proj.bias', 'vit_backbone.blocks.14.norm2.weight', 'vit_backbone.blocks.14.norm2.bias', 'vit_backbone.blocks.14.mlp.fc1.weight', 'vit_backbone.blocks.14.mlp.fc1.bias', 'vit_backbone.blocks.14.mlp.fc2.weight', 'vit_backbone.blocks.14.mlp.fc2.bias', 'vit_backbone.blocks.15.norm1.weight', 'vit_backbone.blocks.15.norm1.bias', 'vit_backbone.blocks.15.attn.qkv.weight', 'vit_backbone.blocks.15.attn.qkv.bias', 'vit_backbone.blocks.15.attn.proj.weight', 'vit_backbone.blocks.15.attn.proj.bias', 'vit_backbone.blocks.15.norm2.weight', 'vit_backbone.blocks.15.norm2.bias', 'vit_backbone.blocks.15.mlp.fc1.weight', 'vit_backbone.blocks.15.mlp.fc1.bias', 'vit_backbone.blocks.15.mlp.fc2.weight', 'vit_backbone.blocks.15.mlp.fc2.bias', 'vit_backbone.blocks.16.norm1.weight', 'vit_backbone.blocks.16.norm1.bias', 'vit_backbone.blocks.16.attn.qkv.weight', 'vit_backbone.blocks.16.attn.qkv.bias', 'vit_backbone.blocks.16.attn.proj.weight', 'vit_backbone.blocks.16.attn.proj.bias', 'vit_backbone.blocks.16.norm2.weight', 'vit_backbone.blocks.16.norm2.bias', 'vit_backbone.blocks.16.mlp.fc1.weight', 'vit_backbone.blocks.16.mlp.fc1.bias', 'vit_backbone.blocks.16.mlp.fc2.weight', 'vit_backbone.blocks.16.mlp.fc2.bias', 'vit_backbone.blocks.17.norm1.weight', 'vit_backbone.blocks.17.norm1.bias', 'vit_backbone.blocks.17.attn.qkv.weight', 'vit_backbone.blocks.17.attn.qkv.bias', 'vit_backbone.blocks.17.attn.proj.weight', 'vit_backbone.blocks.17.attn.proj.bias', 'vit_backbone.blocks.17.norm2.weight', 'vit_backbone.blocks.17.norm2.bias', 'vit_backbone.blocks.17.mlp.fc1.weight', 'vit_backbone.blocks.17.mlp.fc1.bias', 'vit_backbone.blocks.17.mlp.fc2.weight', 'vit_backbone.blocks.17.mlp.fc2.bias', 'vit_backbone.blocks.18.norm1.weight', 'vit_backbone.blocks.18.norm1.bias', 'vit_backbone.blocks.18.attn.qkv.weight', 'vit_backbone.blocks.18.attn.qkv.bias', 'vit_backbone.blocks.18.attn.proj.weight', 'vit_backbone.blocks.18.attn.proj.bias', 'vit_backbone.blocks.18.norm2.weight', 'vit_backbone.blocks.18.norm2.bias', 'vit_backbone.blocks.18.mlp.fc1.weight', 'vit_backbone.blocks.18.mlp.fc1.bias', 'vit_backbone.blocks.18.mlp.fc2.weight', 'vit_backbone.blocks.18.mlp.fc2.bias', 'vit_backbone.blocks.19.norm1.weight', 'vit_backbone.blocks.19.norm1.bias', 'vit_backbone.blocks.19.attn.qkv.weight', 'vit_backbone.blocks.19.attn.qkv.bias', 'vit_backbone.blocks.19.attn.proj.weight', 'vit_backbone.blocks.19.attn.proj.bias', 'vit_backbone.blocks.19.norm2.weight', 'vit_backbone.blocks.19.norm2.bias', 'vit_backbone.blocks.19.mlp.fc1.weight', 'vit_backbone.blocks.19.mlp.fc1.bias', 'vit_backbone.blocks.19.mlp.fc2.weight', 'vit_backbone.blocks.19.mlp.fc2.bias', 'vit_backbone.blocks.20.norm1.weight', 'vit_backbone.blocks.20.norm1.bias', 'vit_backbone.blocks.20.attn.qkv.weight', 'vit_backbone.blocks.20.attn.qkv.bias', 'vit_backbone.blocks.20.attn.proj.weight', 'vit_backbone.blocks.20.attn.proj.bias', 'vit_backbone.blocks.20.norm2.weight', 'vit_backbone.blocks.20.norm2.bias', 'vit_backbone.blocks.20.mlp.fc1.weight', 'vit_backbone.blocks.20.mlp.fc1.bias', 'vit_backbone.blocks.20.mlp.fc2.weight', 'vit_backbone.blocks.20.mlp.fc2.bias', 'vit_backbone.blocks.21.norm1.weight', 'vit_backbone.blocks.21.norm1.bias', 'vit_backbone.blocks.21.attn.qkv.weight', 'vit_backbone.blocks.21.attn.qkv.bias', 'vit_backbone.blocks.21.attn.proj.weight', 'vit_backbone.blocks.21.attn.proj.bias', 'vit_backbone.blocks.21.norm2.weight', 'vit_backbone.blocks.21.norm2.bias', 'vit_backbone.blocks.21.mlp.fc1.weight', 'vit_backbone.blocks.21.mlp.fc1.bias', 'vit_backbone.blocks.21.mlp.fc2.weight', 'vit_backbone.blocks.21.mlp.fc2.bias', 'vit_backbone.blocks.22.norm1.weight', 'vit_backbone.blocks.22.norm1.bias', 'vit_backbone.blocks.22.attn.qkv.weight', 'vit_backbone.blocks.22.attn.qkv.bias', 'vit_backbone.blocks.22.attn.proj.weight', 'vit_backbone.blocks.22.attn.proj.bias', 'vit_backbone.blocks.22.norm2.weight', 'vit_backbone.blocks.22.norm2.bias', 'vit_backbone.blocks.22.mlp.fc1.weight', 'vit_backbone.blocks.22.mlp.fc1.bias', 'vit_backbone.blocks.22.mlp.fc2.weight', 'vit_backbone.blocks.22.mlp.fc2.bias', 'vit_backbone.blocks.23.norm1.weight', 'vit_backbone.blocks.23.norm1.bias', 'vit_backbone.blocks.23.attn.qkv.weight', 'vit_backbone.blocks.23.attn.qkv.bias', 'vit_backbone.blocks.23.attn.proj.weight', 'vit_backbone.blocks.23.attn.proj.bias', 'vit_backbone.blocks.23.norm2.weight', 'vit_backbone.blocks.23.norm2.bias', 'vit_backbone.blocks.23.mlp.fc1.weight', 'vit_backbone.blocks.23.mlp.fc1.bias', 'vit_backbone.blocks.23.mlp.fc2.weight', 'vit_backbone.blocks.23.mlp.fc2.bias', 'vit_backbone.norm.weight', 'vit_backbone.norm.bias', 'vit_backbone.head.weight', 'vit_backbone.head.bias', 'vit_backbone.sequence_encoder.conv1.weight', 'vit_backbone.sequence_encoder.conv1.bias', 'vit_backbone.sequence_encoder.conv2.weight', 'vit_backbone.sequence_encoder.conv2.bias', 'vit_backbone.sequence_encoder.conv3.weight', 'vit_backbone.sequence_encoder.conv3.bias', 'vit_backbone.sequence_encoder.fc.weight', 'vit_backbone.sequence_encoder.fc.bias', 'decoder_map.weight', 'decoder_map.bias', 'map_block.weight', 'map_block.bias'], unexpected_keys=['cls_token', 'pos_embed', 'mask_token', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias', 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.12.mlp.fc1.weight', 'blocks.12.mlp.fc1.bias', 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias', 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.attn.qkv.weight', 'blocks.13.attn.qkv.bias', 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias', 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.mlp.fc1.bias', 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias', 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.attn.qkv.weight', 'blocks.14.attn.qkv.bias', 'blocks.14.attn.proj.weight', 'blocks.14.attn.proj.bias', 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.14.mlp.fc1.weight', 'blocks.14.mlp.fc1.bias', 'blocks.14.mlp.fc2.weight', 'blocks.14.mlp.fc2.bias', 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.attn.qkv.weight', 'blocks.18.attn.qkv.bias', 'blocks.18.attn.proj.weight', 'blocks.18.attn.proj.bias', 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.18.mlp.fc1.weight', 'blocks.18.mlp.fc1.bias', 'blocks.18.mlp.fc2.weight', 'blocks.18.mlp.fc2.bias', 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.attn.qkv.weight', 'blocks.19.attn.qkv.bias', 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias', 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.19.mlp.fc1.weight', 'blocks.19.mlp.fc1.bias', 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias', 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias', 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.attn.qkv.weight', 'blocks.21.attn.qkv.bias', 'blocks.21.attn.proj.weight', 'blocks.21.attn.proj.bias', 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.21.mlp.fc1.weight', 'blocks.21.mlp.fc1.bias', 'blocks.21.mlp.fc2.weight', 'blocks.21.mlp.fc2.bias', 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.attn.qkv.weight', 'blocks.22.attn.qkv.bias', 'blocks.22.attn.proj.weight', 'blocks.22.attn.proj.bias', 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.22.mlp.fc1.weight', 'blocks.22.mlp.fc1.bias', 'blocks.22.mlp.fc2.weight', 'blocks.22.mlp.fc2.bias', 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.attn.qkv.weight', 'blocks.23.attn.qkv.bias', 'blocks.23.attn.proj.weight', 'blocks.23.attn.proj.bias', 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'blocks.23.mlp.fc1.weight', 'blocks.23.mlp.fc1.bias', 'blocks.23.mlp.fc2.weight', 'blocks.23.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_pred.weight', 'decoder_pred.bias', 'decoder_count.weight', 'decoder_count.bias'])
[12:01:39.836470] ==================================================
[12:01:52.929357] Model is configured!
[12:01:52.929506] Learning rate: 0.000141
[12:01:52.929512] Accumulative grad iteration: 2
[12:01:52.929516] Effective batch size: 24
[12:01:52.929520] Base learning rate: 0.001500
[12:01:52.929606] no gradient apply for  decoder_pos_embed
[12:01:52.933224] Optimizer is configured!
[12:01:52.933488] => no checkpoint found at '/storage/ixjl/cse428'
[12:01:52.933600] Start training from epoch 0  to epoch 32
[12:01:52.935410] number of iterations:  1
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
[12:01:57.263653] using ssim
[12:01:57.396965] [GRADIENT HOOK] sequence_embed grad mean: -0.000000, std: 5.136661
[12:01:57.879531] Epoch: [0]  [0/1]  eta: 0:00:04  lr: 0.000000  loss: 0.5771 (0.5771)  embedding_loss: 0.0000 (0.0000)  output_2d_loss: 0.5771 (0.5771)  output_1d_loss: 0.0000 (0.0000)  time: 4.9432  data: 3.2348  max mem: 6111
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
[12:01:58.294908] Epoch: [0] Total time: 0:00:05 (5.3594 s / it)
[12:01:58.296498] Averaged stats: lr: 0.000000  loss: 0.5771 (0.5871)  embedding_loss: 0.0000 (0.0000)  output_2d_loss: 0.5771 (0.5871)  output_1d_loss: 0.0000 (0.0000)
[12:01:58.302474] number of iterations:  1
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701
DEBUG: input_matrix min=0.0, max=0.3010300099849701
DEBUG: max_value = 0.3010300099849701

Traceback (most recent call last):
  File "/storage/ixjl/cse428/finetune.py", line 48, in <module>
    main(args)
  File "/storage/ixjl/cse428/finetune.py", line 26, in main
    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node,  args))
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/storage/ixjl/cse428/finetune/main_worker.py", line 270, in main_worker
    val_stats = val_epoch(model, data_loader_val, device, epoch,
  File "/storage/ixjl/cse428/finetune/val_epoch.py", line 29, in val_epoch
    output_embedding, output_2d, output_1d = model(input_matrix, total_count, sequence_data)
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/storage/ixjl/cse428/ixjl_cse428/lib64/python3.9/site-packages/torch/nn/parallel/distributed.py", line 692, in forward
    if self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).