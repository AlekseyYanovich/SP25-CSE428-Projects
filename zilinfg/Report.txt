Project Report: Using Gemini to Defend Against Scorpius Generated Medical Abstracts

Student: Zilin Fang
Date: June 2, 2025
Paper: Poisoning medical knowledge using large language models

1. What We Tried to Do (Introduction & Goal)

"Scorpius" showed that poisoning medicak KGs are a real risk. So, this project was about building a "defender" using Google's Gemini AI. Our main aim was to see if Gemini could tell the difference between real scientific medical abstracts and generated fake ones, to help find the information we can trust.

2. How We Did It (Method & Process)

The Test Data: We used a set of 224 medical abstracts for our test:
100 were real ones from PubMed (we called these "real").
124 were fakes, made to be like the "Scorpius" attack (we called these "gen").
We mixed all 224 up randomly before we started.
How the Defender Worked (Gemini Tournament):
We set up a tournament where Gemini compared two abstracts at a time. (We used [mention model if you recall, e.g., Gemini 1.5 Flash]).
For each pair, we asked Gemini to pick the one that seemed more like a real, scientific paper.
The abstract Gemini picked got a "win."
Each abstract went through this 5 times (K=5), getting compared against 5 different opponents.
After all the rounds, we ranked the abstracts by how many wins they got and picked the top 100.

3. What We Found (Findings & Results)

Here’s what the defender picked in its top 100:

Real Abstracts: 57
Generated (Fake) Abstracts: 43
Compared to Guessing: If we just picked 100 abstracts randomly from our mixed pool, we'd expect to get about 50 real and 50 fake ones.

4. Thinking About the Results (Analysis & Discussion)

So, what does this tell us? The Gemini defender did a better job than just random guessing. It managed to pick more real abstracts (57%) than the 50% we'd expect if we didn't use the defender. This is good – it shows that Gemini can help tell these abstracts apart.

However, it wasn't perfect. The defender still included 43 fake abstracts in its top 100, and it missed 43 of the real ones. This shows that the fake abstracts are pretty convincing and tough to spot. Doing 5 comparison rounds (K=5) helped Gemini get a sense of things, but it wasn't enough to perfectly separate them.

5. What We Could Try Next (Proposed Future Actions)

To make the defender better, here are some ideas:

Look Closer at Mistakes: Really dig into why some fake abstracts got picked and why some real ones were left out. This could help us write better instructions (prompts) for Gemini.
Try Some Tweaks:
See if doing more comparison rounds for each abstract (a higher K) makes a difference.
Test if a more powerful Gemini model (like Gemini Pro) does a better job.
Smarter Tournament Ideas:
Maybe Gemini could give a "confidence score" for its choice, not just a "win." This might give us a better way to rank them.
We could try a two-step tournament: a quick first round to filter some out, then a more focused second round for the ones that are hard to tell apart.
Mix in Other Methods: Add other ways to check the abstracts, like looking for writing style patterns that are common in AI-generated text.
Test It More Broadly: See how well the defender works if there are more or fewer fake abstracts in the mix, or if the fakes are made using different methods.


6. In Short (Conclusion)

This project built a working Gemini defender that's better than just guessing when it comes to finding real medical abstracts. It shows that this approach has promise. The results also show that it's a hard problem, but this work is a good starting point for more research into how AI can help protect Knoledge Graphs and fact checking in general.